{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navinor/dl-survey-live-lab-2025/blob/main/01_baseline_random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01 — Baseline Random Forest (Student)\n",
        "\n",
        "**Today we will:**\n",
        "1) Load the Adult dataset (classification: `income`).\n",
        "2) Split train/test **first** (avoid leakage).\n",
        "3) Handle missing values (numeric: mean; categorical: most frequent).\n",
        "4) Encode categorical features:\n",
        "   - Label encode **one** column (`sex`)\n",
        "   - One-hot encode the rest\n",
        "5) (Practice) Scale numeric features (StandardScaler).\n",
        "6) Train a **RandomForestClassifier** and evaluate it.\n",
        "7) **Deep dive** into the RF object: attributes & methods.\n",
        "8) Try a few hyperparameters and record results in your `students/experiment_log.md`.\n",
        "9) Peek at **GridSearchCV** (teacher-led demo).\n",
        "\n",
        "> As you work: add short notes in your experiment log (Goal → Setup → Results → Reflection).\n"
      ],
      "metadata": {
        "id": "7kEq4maoG9eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "V4h1ZDmyHIrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Setup\n",
        "!pip -q install scikit-learn pandas matplotlib seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "print(\"Ready.\")\n"
      ],
      "metadata": {
        "id": "QzRQw9sKHCK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "XA0TOkgFHMCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Load the Adult dataset from OpenML\n",
        "adult = fetch_openml(name=\"adult\", version=2, as_frame=True)\n",
        "df = adult.frame.copy()\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "tMjwBsLKHPvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Is this a classification or regression problem? **Classification Problem**\n",
        "\n",
        "\n",
        "**Target column:** Class\n"
      ],
      "metadata": {
        "id": "fCor1H6KHaU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose columns & split FIRST"
      ],
      "metadata": {
        "id": "1nGA1kAmHdAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Choose columns (keep it small for speed)\n",
        "numeric_features = [\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]\n",
        "categorical_features = [\"workclass\", \"marital-status\", \"occupation\", \"sex\", \"native-country\"]\n",
        "target_col = \"class\"\n",
        "\n",
        "use_cols = numeric_features + categorical_features + [target_col]\n",
        "df = df[use_cols].copy()\n",
        "\n",
        "# 3) Split FIRST (to avoid leakage)\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "X_train.shape, X_test.shape\n"
      ],
      "metadata": {
        "id": "XrQpnKQyHkKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why split first?**  \n",
        "So that we can ensure that the test set remains an unseen proxy for new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "3jkmTckzH6b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle missing values"
      ],
      "metadata": {
        "id": "XPllvxCZIBOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Impute missing values\n",
        "# Numeric → mean (try median later if you like)\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train[numeric_features]),\n",
        "                           columns=numeric_features, index=X_train.index)\n",
        "X_test_num  = pd.DataFrame(num_imputer.transform(X_test[numeric_features]),\n",
        "                           columns=numeric_features, index=X_test.index)\n",
        "\n",
        "# Categorical → most frequent\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "X_train_cat_raw = pd.DataFrame(cat_imputer.fit_transform(X_train[categorical_features]),\n",
        "                               columns=categorical_features, index=X_train.index)\n",
        "X_test_cat_raw  = pd.DataFrame(cat_imputer.transform(X_test[categorical_features]),\n",
        "                               columns=categorical_features, index=X_test.index)\n"
      ],
      "metadata": {
        "id": "FMli46S0IFuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode categoricals"
      ],
      "metadata": {
        "id": "CFuCQDKOITzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Encode categoricals\n",
        "# Label-encode ONE column ('sex'); one-hot the rest\n",
        "label_encode_col = \"sex\"\n",
        "\n",
        "# Copies\n",
        "X_train_processed = X_train_cat_raw.copy()\n",
        "X_test_processed  = X_test_cat_raw.copy()\n",
        "\n",
        "# LabelEncoder: fit on train, apply to test\n",
        "le = LabelEncoder()\n",
        "X_train_processed[label_encode_col] = le.fit_transform(X_train_processed[label_encode_col])\n",
        "X_test_processed[label_encode_col]  = le.transform(X_test_processed[label_encode_col])\n",
        "\n",
        "# One-hot all other categorical columns\n",
        "onehot_cols = [c for c in X_train_processed.columns if c != label_encode_col]\n",
        "\n",
        "# Version-safe: sparse_output (>=1.2) vs sparse (<1.2)\n",
        "try:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "except TypeError:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "X_train_ohe_array = ohe.fit_transform(X_train_processed[onehot_cols])\n",
        "X_test_ohe_array  = ohe.transform(X_test_processed[onehot_cols])\n",
        "\n",
        "ohe_feature_names = ohe.get_feature_names_out(onehot_cols)\n",
        "X_train_ohe = pd.DataFrame(X_train_ohe_array, columns=ohe_feature_names)\n",
        "X_test_ohe  = pd.DataFrame(X_test_ohe_array,  columns=ohe_feature_names)\n",
        "\n",
        "# Combine label-encoded + one-hot (reset indices for alignment)\n",
        "label_encoded_train = X_train_processed[[label_encode_col]].reset_index(drop=True)\n",
        "label_encoded_test  = X_test_processed[[label_encode_col]].reset_index(drop=True)\n",
        "X_train_cat = pd.concat([label_encoded_train, X_train_ohe.reset_index(drop=True)], axis=1)\n",
        "X_test_cat  = pd.concat([label_encoded_test,  X_test_ohe.reset_index(drop=True)],  axis=1)\n",
        "\n",
        "X_train_cat.shape, X_test_cat.shape\n"
      ],
      "metadata": {
        "id": "5qFdLucQIW0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflect:**  \n",
        "- When might **label encoding** be risky? **Answer: when you don't want the model to incorrectly identify an order in the data**\n",
        "- Why is **one-hot** often safer for models like Logistic Regression or Neural Nets? **Answer: one-hot encoding is often safer from these models, because it minimizes the chances of these models picking up an order in the data**\n"
      ],
      "metadata": {
        "id": "7WaPWu0jIiOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale numeric features (practice)"
      ],
      "metadata": {
        "id": "-jXtk2ZPItIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Scale numeric features (practice—even if RF doesn’t need it)\n",
        "scaler = StandardScaler()\n",
        "X_train_num_scaled = pd.DataFrame(scaler.fit_transform(X_train_num), columns=numeric_features)\n",
        "X_test_num_scaled  = pd.DataFrame(scaler.transform(X_test_num),   columns=numeric_features)\n",
        "\n",
        "# Assemble final matrices (reset indices to align rows)\n",
        "X_train_final = pd.concat([X_train_num_scaled.reset_index(drop=True),\n",
        "                           X_train_cat.reset_index(drop=True)], axis=1)\n",
        "X_test_final  = pd.concat([X_test_num_scaled.reset_index(drop=True),\n",
        "                           X_test_cat.reset_index(drop=True)],  axis=1)\n",
        "\n",
        "X_train_final.shape, X_test_final.shape\n"
      ],
      "metadata": {
        "id": "kU1XGWMlIoe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When would you prefer**:\n",
        "- `MinMaxScaler` ?\n",
        "- `RobustScaler` ?\n"
      ],
      "metadata": {
        "id": "vzO9iUMfJyiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train RF + Evaluate"
      ],
      "metadata": {
        "id": "zXu_WfCTJ5fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Train a baseline Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_final, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test_final)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=rf.classes_)\n",
        "sns.heatmap(pd.DataFrame(cm, index=[f\"true_{c}\" for c in rf.classes_],\n",
        "                            columns=[f\"pred_{c}\" for c in rf.classes_]),\n",
        "            annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NrrkQWV1J9Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep dive: RF Object"
      ],
      "metadata": {
        "id": "Ylz6u_gJKHE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Inspect the RF object: attributes & methods\n",
        "print(\"Number of trees:\", rf.n_estimators)\n",
        "print(\"Classes:\", rf.classes_)\n",
        "print(\"Max depth setting:\", rf.max_depth)\n",
        "\n",
        "# Feature importances (top 10)\n",
        "importances = pd.Series(rf.feature_importances_, index=X_train_final.columns).sort_values(ascending=False)\n",
        "importances.head(10)\n"
      ],
      "metadata": {
        "id": "AbIhhT1_KMiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict vs predict_proba\n",
        "pred_label = rf.predict(X_test_final[:5])\n",
        "pred_prob  = rf.predict_proba(X_test_final[:5])\n",
        "pred_label, pred_prob\n"
      ],
      "metadata": {
        "id": "3IC-IU79KZ3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain in your own words:**  \n",
        "- What’s the difference between `predict` and `predict_proba`?  \n",
        "- Which would you show in an app UI, and why?\n"
      ],
      "metadata": {
        "id": "68Ks5JhwKlLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tinkering with Hyperparameters"
      ],
      "metadata": {
        "id": "wpocxi6lKrDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Tinker: try one change, re-run report, log in experiment_log.md\n",
        "rf2 = RandomForestClassifier(\n",
        "    n_estimators=200,   # try 50 / 100 / 200\n",
        "    max_depth=None,     # try 10 / 20 / None\n",
        "    min_samples_split=2,  # try 2 / 5\n",
        "    random_state=42\n",
        ")\n",
        "rf2.fit(X_train_final, y_train)\n",
        "print(classification_report(y_test, rf2.predict(X_test_final)))\n"
      ],
      "metadata": {
        "id": "E0lX_r_jKzsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GridSearchCV"
      ],
      "metadata": {
        "id": "A3mDYVwRLIft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "}\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                    param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid.fit(X_train_final, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV accuracy:\", round(grid.best_score_, 4))\n",
        "print(\"Test accuracy with best params:\", round(grid.best_estimator_.score(X_test_final, y_test), 4))\n"
      ],
      "metadata": {
        "id": "4JhC3gCwLHVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🏠 Homework: GridSearchCV vs RandomizedSearchCV\n",
        "\n",
        "Today we used **GridSearchCV** to systematically test a small parameter grid.  \n",
        "But GridSearch gets expensive if the parameter space is large.  \n",
        "An alternative is **RandomizedSearchCV**: it samples combinations at random.\n",
        "\n",
        "**Task:**  \n",
        "1. Run the provided code that compares GridSearchCV and RandomizedSearchCV.  \n",
        "2. Note which one is faster, and whether they found similar/better hyperparameters.  \n",
        "3. Add your reflection in `students/experiment_log.md` under \"Run 2\".\n"
      ],
      "metadata": {
        "id": "d-Tm5BGQMkSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n0JB2FXnMkEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Define parameter distributions for RandomizedSearch\n",
        "param_dist = {\n",
        "    \"n_estimators\": randint(50, 300),\n",
        "    \"max_depth\": [None, 10, 20, 30],\n",
        "    \"min_samples_split\": randint(2, 10)\n",
        "}\n",
        "\n",
        "# GridSearch (small grid)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                    param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "random_search = RandomizedSearchCV(RandomForestClassifier(random_state=42),\n",
        "                                   param_dist, n_iter=10, cv=3,\n",
        "                                   scoring=\"accuracy\", n_jobs=-1,\n",
        "                                   random_state=42)\n",
        "\n",
        "print(\"Running GridSearchCV...\")\n",
        "grid.fit(X_train_final, y_train)\n",
        "\n",
        "print(\"Running RandomizedSearchCV...\")\n",
        "random_search.fit(X_train_final, y_train)\n",
        "\n",
        "print(\"Best params (GridSearch):\", grid.best_params_)\n",
        "print(\"Best score (GridSearch):\", round(grid.best_score_, 4))\n",
        "\n",
        "print(\"Best params (RandomizedSearch):\", random_search.best_params_)\n",
        "print(\"Best score (RandomizedSearch):\", round(random_search.best_score_, 4))\n"
      ],
      "metadata": {
        "id": "qeSuqgjPMo2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection Questions\n",
        "- Which method finished faster? Why?  \n",
        "- Did they find similar or different best parameters?  \n",
        "- When would you choose GridSearchCV vs RandomizedSearchCV in practice?  \n"
      ],
      "metadata": {
        "id": "JXhJMbowMvCe"
      }
    }
  ]
}